{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0-rc1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "# graph execution mode下可以不定义input_shape，会根据feed的数据自动判断\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),  \n",
    "# Add another:\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a softmax layer with 10 output units:\n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "# Configure a model for categorical classification.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 189us/sample - loss: 2.3151 - categorical_accuracy: 0.0920 - val_loss: 2.3201 - val_categorical_accuracy: 0.0900\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 41us/sample - loss: 2.3078 - categorical_accuracy: 0.1050 - val_loss: 2.3310 - val_categorical_accuracy: 0.0500\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 2.3087 - categorical_accuracy: 0.1020 - val_loss: 2.3827 - val_categorical_accuracy: 0.1200\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 33us/sample - loss: 2.3104 - categorical_accuracy: 0.1010 - val_loss: 2.3088 - val_categorical_accuracy: 0.0900\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 2.2973 - categorical_accuracy: 0.1130 - val_loss: 2.3823 - val_categorical_accuracy: 0.1300\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 33us/sample - loss: 2.2932 - categorical_accuracy: 0.1430 - val_loss: 2.3202 - val_categorical_accuracy: 0.1500\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 2.2736 - categorical_accuracy: 0.1540 - val_loss: 2.2927 - val_categorical_accuracy: 0.1400\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 2.2513 - categorical_accuracy: 0.1660 - val_loss: 2.3034 - val_categorical_accuracy: 0.1500\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 43us/sample - loss: 2.2309 - categorical_accuracy: 0.1680 - val_loss: 2.4552 - val_categorical_accuracy: 0.1200\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 2.2145 - categorical_accuracy: 0.1840 - val_loss: 2.3373 - val_categorical_accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f206f4ce6d8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_one_hot_labels(shape):\n",
    "  n, n_class = shape\n",
    "  classes = np.random.randint(0, n_class, n)\n",
    "  labels = np.zeros((n, n_class))\n",
    "  labels[np.arange(n), classes] = 1\n",
    "  return labels\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = random_one_hot_labels((1000, 10))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = random_one_hot_labels((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit with data wrapped with tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 30 steps, validate on 3 steps\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6942 - categorical_accuracy: 0.4021 - val_loss: 3.0471 - val_categorical_accuracy: 0.1354\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.5593 - categorical_accuracy: 0.4380 - val_loss: 3.0045 - val_categorical_accuracy: 0.0729\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.5355 - categorical_accuracy: 0.4626 - val_loss: 3.2579 - val_categorical_accuracy: 0.1146\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.4853 - categorical_accuracy: 0.4615 - val_loss: 3.6905 - val_categorical_accuracy: 0.0833\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.4208 - categorical_accuracy: 0.4968 - val_loss: 3.5830 - val_categorical_accuracy: 0.0938\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3603 - categorical_accuracy: 0.5310 - val_loss: 3.5988 - val_categorical_accuracy: 0.0625\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3318 - categorical_accuracy: 0.5342 - val_loss: 3.4248 - val_categorical_accuracy: 0.0625\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.2366 - categorical_accuracy: 0.5620 - val_loss: 3.9430 - val_categorical_accuracy: 0.0521\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1696 - categorical_accuracy: 0.5919 - val_loss: 4.3110 - val_categorical_accuracy: 0.1042\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 972us/step - loss: 1.1211 - categorical_accuracy: 0.6197 - val_loss: 4.4700 - val_categorical_accuracy: 0.1042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f21afc559e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "# 使用tf.data.Dataset训练必须加上steps_per_epoch\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30,\n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 40us/sample - loss: 4.2967 - categorical_accuracy: 0.0890\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 4.2987 - categorical_accuracy: 0.0885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.298702851931254, 0.088541664]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "data = np.random.random((1000, 32))\n",
    "labels = random_one_hot_labels((1000, 10))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "model.evaluate(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model with functinal API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 2.3229 - acc: 0.1010\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 31us/sample - loss: 2.3184 - acc: 0.1030\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 2.3107 - acc: 0.0830\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 2.3043 - acc: 0.1180\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 2.2943 - acc: 0.1090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20b071deb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(32, )) # Returns a placeholder tensor\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model with subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 2.3238 - acc: 0.1060\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.3227 - acc: 0.1050\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.3191 - acc: 0.1020\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 2.3131 - acc: 0.1040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 2.3085 - acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20d4126c50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(MyModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.dense_1 = layers.Dense(32, activation='relu')\n",
    "    self.dense_2 = layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define your forward pass here,\n",
    "    # using layers you previously defined (in `__init__`).\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    # You need to override this function if you want to use the subclassed model\n",
    "    # as part of a functional-style model.\n",
    "    # Otherwise, this method is optional.\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.num_classes\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 2.3094 - acc: 0.0990\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 24us/sample - loss: 2.3081 - acc: 0.0960\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 2.3056 - acc: 0.0960\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 2.3034 - acc: 0.1190\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 2.3005 - acc: 0.1130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20d4240780>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# must implement __init__,build,call,compute_output_shape, optionally get_config and class methods from_config\n",
    "class MyLayer(layers.Layer):\n",
    "\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    self.output_dim = output_dim\n",
    "    super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "    # Create a trainable weight variable for this layer.\n",
    "    self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=shape,\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "    # Make sure to call the `build` method at the end\n",
    "    super(MyLayer, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.output_dim\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super(MyLayer, self).get_config()\n",
    "    base_config['output_dim'] = self.output_dim\n",
    "    return base_config\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    MyLayer(10),\n",
    "    layers.Activation('softmax')])\n",
    "\n",
    "# The compile step specifies the training configuration\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 2.2980 - acc: 0.1270 - val_loss: 2.3065 - val_acc: 0.0800\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 32us/sample - loss: 2.2954 - acc: 0.1210 - val_loss: 2.3081 - val_acc: 0.0900\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 33us/sample - loss: 2.2933 - acc: 0.1320 - val_loss: 2.3079 - val_acc: 0.0800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20b07730b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 2.3262 - acc: 0.0990 - val_loss: 2.3205 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 2.3100 - acc: 0.1150 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 2.2984 - acc: 0.1160 - val_loss: 2.3160 - val_acc: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 2.2886 - acc: 0.1310 - val_loss: 2.3152 - val_acc: 0.1000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 36us/sample - loss: 2.2809 - acc: 0.1370 - val_loss: 2.3172 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f209255e940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, batch_size=32, epochs=5,\n",
    "          validation_data=(val_data, val_labels))\n",
    "\n",
    "# Save weights to a TensorFlow Checkpoint file\n",
    "model.save_weights('./weights/my_model')\n",
    "\n",
    "# Restore the model's state,\n",
    "# this requires a model with the same architecture.\n",
    "model.load_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to a HDF5 file\n",
    "model.save_weights('my_model.h5', save_format='h5')\n",
    "\n",
    "# Restore the model's state\n",
    "model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model configuration without any wights, equivalent to save model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_4\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_13\", \"trainable\": true, \"batch_input_shape\": [null, 32], \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null, \"dtype\": \"float32\"}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {\"dtype\": \"float32\"}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_14\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null, \"dtype\": \"float32\"}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {\"dtype\": \"float32\"}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize a model to JSON format\n",
    "json_string = model.to_json()\n",
    "json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backend': 'tensorflow',\n",
      " 'class_name': 'Sequential',\n",
      " 'config': {'layers': [{'class_name': 'Dense',\n",
      "                        'config': {'activation': 'relu',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'batch_input_shape': [None, 32],\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {'dtype': 'float32'}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'dtype': 'float32',\n",
      "                                                                     'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_13',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 64,\n",
      "                                   'use_bias': True}},\n",
      "                       {'class_name': 'Dense',\n",
      "                        'config': {'activation': 'softmax',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {'dtype': 'float32'}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'dtype': 'float32',\n",
      "                                                                     'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_14',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 10,\n",
      "                                   'use_bias': True}}],\n",
      "            'name': 'sequential_4'},\n",
      " 'keras_version': '2.2.4-tf'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "pprint.pprint(json.loads(json_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/AI/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model (newly initialized) from the JSON:\n",
    "fresh_model = tf.keras.models.model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "  layers:\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: relu\n",
      "      activity_regularizer: null\n",
      "      batch_input_shape: !!python/tuple [null, 32]\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {dtype: float32}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config: {dtype: float32, seed: null}\n",
      "      kernel_regularizer: null\n",
      "      name: dense_13\n",
      "      trainable: true\n",
      "      units: 64\n",
      "      use_bias: true\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: softmax\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {dtype: float32}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config: {dtype: float32, seed: null}\n",
      "      kernel_regularizer: null\n",
      "      name: dense_14\n",
      "      trainable: true\n",
      "      units: 10\n",
      "      use_bias: true\n",
      "  name: sequential_4\n",
      "keras_version: 2.2.4-tf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Serialize a model to YAML\n",
    "yaml_string = model.to_yaml()\n",
    "print(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_model = tf.keras.models.model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save entired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 2.3435 - acc: 0.0870\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.3187 - acc: 0.1010\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 35us/sample - loss: 2.3090 - acc: 0.1080\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 2.3002 - acc: 0.1060\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.2907 - acc: 0.1340\n"
     ]
    }
   ],
   "source": [
    "# Create a trivial model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, batch_size=32, epochs=5)\n",
    "\n",
    "\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f20922fcf60>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f20922fc320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='model_dir/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: model_dir/keras/keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 4 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6927031, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 10 into model_dir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.70273423.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f2090202ba8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "def input_fn():\n",
    "  x = np.random.random((1024, 10))\n",
    "  y = np.random.randint(2, size=(1024, 1))\n",
    "  x = tf.cast(x, tf.float32)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "  dataset = dataset.repeat(10)\n",
    "  dataset = dataset.batch(32)\n",
    "  return dataset\n",
    "\n",
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "\n",
    "keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "  keras_model=model,\n",
    "  config=config,\n",
    "  model_dir='model_dir')\n",
    "\n",
    "keras_estimator.train(input_fn=input_fn, steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI] *",
   "language": "python",
   "name": "conda-env-AI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
